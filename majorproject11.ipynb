{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b5312b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import PIL\n",
    "from PIL import Image,ImageFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bcdc9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import image\n",
    "from keras.layers import BatchNormalization,Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D,InputLayer,Input,UpSampling2D\n",
    "from keras_preprocessing.image import ImageDataGenerator,load_img,img_to_array\n",
    "from keras.preprocessing import image,sequence\n",
    "from skimage.io import imsave\n",
    "from keras.callbacks import TensorBoard\n",
    "from skimage.color import rgb2lab,lab2rgb,gray2rgb\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from cv2 import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6406fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'images/colorization/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "cd528ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 images belonging to 1 classes.\n",
      "(4, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "#Resize images, if needed\n",
    "train = train_datagen.flow_from_directory(path, \n",
    "                                          target_size=(256, 256), \n",
    "                                          batch_size=340, \n",
    "                                          class_mode=None)\n",
    "X_train, X_test = train_test_split(train[0], test_size=0.2,random_state=1)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1a6e1a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 256, 256, 1)\n",
      "(12, 256, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "X =[]\n",
    "Y =[]\n",
    "for img in X_train:\n",
    "  try:\n",
    "      lab = rgb2lab(img)\n",
    "      X.append(lab[:,:,0]) \n",
    "      Y.append(lab[:,:,1:] / 128) #A and B values range from -127 to 128, \n",
    "      #so we divide the values by 128 to restrict values to between -1 and 1.\n",
    "  except:\n",
    "     print('error')\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "X = X.reshape(X.shape+(1,)) #dimensions to be the same for X and Y\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9a73130c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_127 (Conv2D)         (None, 128, 128, 64)      640       \n",
      "                                                                 \n",
      " conv2d_128 (Conv2D)         (None, 128, 128, 128)     73856     \n",
      "                                                                 \n",
      " conv2d_129 (Conv2D)         (None, 64, 64, 128)       147584    \n",
      "                                                                 \n",
      " conv2d_130 (Conv2D)         (None, 64, 64, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_131 (Conv2D)         (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_132 (Conv2D)         (None, 32, 32, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_133 (Conv2D)         (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_134 (Conv2D)         (None, 32, 32, 256)       1179904   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,827,200\n",
      "Trainable params: 5,827,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2, input_shape=(256, 256, 1)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3,3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a6dd150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_127 (Conv2D)         (None, 128, 128, 64)      640       \n",
      "                                                                 \n",
      " conv2d_128 (Conv2D)         (None, 128, 128, 128)     73856     \n",
      "                                                                 \n",
      " conv2d_129 (Conv2D)         (None, 64, 64, 128)       147584    \n",
      "                                                                 \n",
      " conv2d_130 (Conv2D)         (None, 64, 64, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_131 (Conv2D)         (None, 32, 32, 256)       590080    \n",
      "                                                                 \n",
      " conv2d_132 (Conv2D)         (None, 32, 32, 512)       1180160   \n",
      "                                                                 \n",
      " conv2d_133 (Conv2D)         (None, 32, 32, 512)       2359808   \n",
      "                                                                 \n",
      " conv2d_134 (Conv2D)         (None, 32, 32, 256)       1179904   \n",
      "                                                                 \n",
      " conv2d_135 (Conv2D)         (None, 32, 32, 128)       295040    \n",
      "                                                                 \n",
      " up_sampling2d_33 (UpSamplin  (None, 64, 64, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_136 (Conv2D)         (None, 64, 64, 64)        73792     \n",
      "                                                                 \n",
      " up_sampling2d_34 (UpSamplin  (None, 128, 128, 64)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_137 (Conv2D)         (None, 128, 128, 32)      18464     \n",
      "                                                                 \n",
      " conv2d_138 (Conv2D)         (None, 128, 128, 16)      4624      \n",
      "                                                                 \n",
      " conv2d_139 (Conv2D)         (None, 128, 128, 2)       290       \n",
      "                                                                 \n",
      " up_sampling2d_35 (UpSamplin  (None, 256, 256, 2)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,219,410\n",
      "Trainable params: 6,219,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 14). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: nikka\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: nikka\\assets\n"
     ]
    }
   ],
   "source": [
    "model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(16, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.compile(optimizer='adam', loss='mse' , metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.save(\"nikka\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ccd4821a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 5s 5s/step - loss: 0.1144 - accuracy: 0.4045 - val_loss: 0.8041 - val_accuracy: 0.8393\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8451 - accuracy: 0.7964 - val_loss: 0.8850 - val_accuracy: 0.8448\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9520 - accuracy: 0.7433 - val_loss: 0.8721 - val_accuracy: 0.7382\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9270 - accuracy: 0.5776 - val_loss: 1.1856 - val_accuracy: 0.1564\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.1011 - accuracy: 0.2032 - val_loss: 1.0843 - val_accuracy: 0.1844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21916b5d690>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y,validation_split=0.1, epochs=5, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fa3494ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 256, 256, 1)\n",
      "(4, 256, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "X_tests =[]\n",
    "Y_tests =[]\n",
    "for img in X_test:\n",
    "  try:\n",
    "      lab = rgb2lab(img)\n",
    "      X_tests.append(lab[:,:,0]) \n",
    "      Y_tests.append(lab[:,:,1:] / 128) #A and B values range from -127 to 128, \n",
    "      #so we divide the values by 128 to restrict values to between -1 and 1.\n",
    "  except:\n",
    "     print('error')\n",
    "X_tests = np.array(X_tests)\n",
    "Y_tests= np.array(Y_tests)\n",
    "X_tests = X_tests.reshape(X_tests.shape+(1,)) #dimensions to be the same for X and Y\n",
    "print(X_tests.shape)\n",
    "print(Y_tests.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "baf941bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n",
      "(256, 256, 1)\n",
      "8/8 [==============================] - 1s 93ms/step\n",
      "(256, 256, 8, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (256,256,8,2) into shape (256,256,2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [180], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m result\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m     15\u001b[0m result[:,:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m l\n\u001b[1;32m---> 16\u001b[0m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m ab \n\u001b[0;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(lab2rgb(result))\n\u001b[0;32m     18\u001b[0m imsave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/Acer/OneDrive/Desktop/New folder (2)/New folder/jpt/model1 \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39mi, lab2rgb(result))\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (256,256,8,2) into shape (256,256,2)"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for img in X_test[i]:\n",
    "    lab = rgb2lab(X_test[i])\n",
    "    #print(lab.shape)\n",
    "    #print(lab.shape)\n",
    "    l= lab[:,:,0]\n",
    "    print(l.shape)\n",
    "    lo= l.reshape(l.shape+(1,))\n",
    "    print(lo.shape)\n",
    "    ab = model.predict(lo)\n",
    "    #print(vggpred.shape)\n",
    "    ab = ab*128\n",
    "    print(ab.shape)\n",
    "    result= np.zeros((256, 256, 3))\n",
    "    result[:,:,0] = l\n",
    "    result[:,:,1:] = ab \n",
    "    plt.imshow(lab2rgb(result))\n",
    "    imsave('C:/Users/Acer/OneDrive/Desktop/New folder (2)/New folder/jpt/model1 %i.jpg'%i, lab2rgb(result))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba4a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abe8cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90eb28d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
